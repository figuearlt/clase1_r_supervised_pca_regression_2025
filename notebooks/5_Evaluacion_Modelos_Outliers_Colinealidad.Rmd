---
title: "Notebook_Clase5_y_6_Evaluacion_Modelos_Outliers_Colinealidad"
author: "Diego Figueroa"
date: "2025-06-01"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### El problema Fundamental en Ciencia de Datos para Modelos Predictivos

#### Trade-off entre Sesgo y Varianza

En modelos predictivos, buscamos minimizar el error total. Este error se puede descomponer en tres partes:

$$
\mathbb{E}\left[(\hat{f}(x) - f(x))^2\right] = \left(\text{Bias}[\hat{f}(x)]\right)^2 + \text{Var}[\hat{f}(x)] + \sigma^2
$$

- **Sesgo (Bias)**: Error por hacer suposiciones demasiado simples sobre los datos. Conduce a *underfitting*.
- **Varianza (Variance)**: Error por hacer el modelo muy sensible a las variaciones del conjunto de entrenamiento. Conduce a *overfitting*.
- **\( \sigma^2 \)**: Error irreducible debido al ruido en los datos.

---

#### Gráfico conceptual (para agregar en un informe)



```{r, echo=FALSE, out.width="80%", fig.align="center"}
knitr::include_graphics("C:/Users/diego/OneDrive/Escritorio/Diplomado Data Science/Diplomado PUCV/r_supervisedmodelling_pca_regression_2025/resources/bias_variance_tradeoff.png")

```

#### Ejemplos:
- **Modelo con alto sesgo**: Regresión lineal simple sobre datos no lineales.
- **Modelo con alta varianza**: Árbol de decisión sin podar o regresión polinomial de alto grado.

\newpage

### Problemas comunes en regresión lineal múltiple

---

#### 1. Outliers

##### Diagnóstico:
- Visualización con gráficos de residuos.
- Distancia de Cook \( D_i \):
  
  $$
  D_i = \frac{(e_i^2)}{p \cdot \hat{\sigma}^2} \cdot \left( \frac{h_{ii}}{(1 - h_{ii})^2} \right)
  $$

##### Soluciones:
- Transformaciones (log, raíz cuadrada).
- Regresión robusta (ej: `rlm()` en R).
- Winsorización de extremos.

---

#### 2. Multicolinealidad

##### Diagnóstico:
- VIF (Variance Inflation Factor):

  $$
  \text{VIF}_j = \frac{1}{1 - R_j^2}
  $$

- Si \( \text{VIF}_j > 5 \): hay evidencia de multicolinealidad.

##### Soluciones:
- Eliminar variables altamente correlacionadas.
- Transformar o combinar predictores (ej: PCA).
- Usar **Ridge Regression**:

  $$
  \min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
  $$
### Ejemplo Simulado

```{r}
eps<-rnorm(90)
eps2<-rnorm(90)*0.01
x1<-rnorm(90)*2
x2<-0.5*x1+eps2
x3<-rnorm(90)
y=3*x1+8*x2+2*x3+3+eps
lin<-lm(y~x1+x2+x3)
summary(lin)
cor.test(x1,x2)

X<-matrix(1,90,4)
X[,2]<-x1
X[,3]<-x2
X[,4]<-x3
solve(t(X)%*%X)
```
\newpage

### Regularización: LASSO, Ridge y Elastic Net

La regularización es una técnica que permite controlar la complejidad del modelo y evitar el sobreajuste (overfitting) penalizando los coeficientes de la regresión. A continuación, se detallan tres métodos ampliamente utilizados:

---

#### Ridge Regression (Regresión de cresta)

Penaliza la suma de los cuadrados de los coeficientes. La función objetivo a minimizar es:

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\}
$$

Donde:
- \( \lambda \) es el parámetro de penalización,
- \( \beta_j \) son los coeficientes del modelo,
- \( y_i \) es la variable dependiente,
- \( \hat{y}_i \) es la predicción del modelo.

Buscamos solucionar:
- La colinealidad de baja dimensión (pocos predictores)

##### Caso Ejemplo: MTCARS

```{r}
mtcars2 <- within(mtcars, {
  vs <- factor(vs, labels = c("V", "S"))
  am <- factor(am, labels = c("automatic", "manual"))
  cyl <- factor(cyl, labels = c("4", "6","8"))
  gear <- factor(gear, labels = c("3", "4","5"))
  carb <- factor(carb, labels = c("1", "2","3","4","6","8"))
})
#Comentario:
#Aunque las variables Cyl, gear y carb tienen el sentido del orden, lo defino 
#como "factor" porque cilindros de un auto, no se puede comparar con cilindros de otra.

#Se puede incorporar "ordered" cuando estamos con variables que tienen espacio regular
#entre niveles (pocos niveles) tal como temperatura= 20, 40 y 60°C, concentracion de
#producto 10, 20 y 30 g/L. En las salidas se entrega resultados con regresión polinomial
lin<-lm(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2)
summary(lin)

```
```{r}
cor(mtcars[,c(-2,-8-9-10,-11)])
```
Hay predictores que tienen alta correlación entre predictores como hp y wt

```{r}

library(car)
car::vif(lin)#calculo de los VIF para cada variable tomando en cuenta los grados de libertad de las variables categoricas
cal.vif<-car::vif(lin)
cal.vif[,3]<-cal.vif[,3]^2
cal.vif#hay que considerar el cuadrado para poder ocupar las mismas reglas arbitrarias

```

```{r}
###################################################
#1a solución: eliminar las variables problematicas#
###################################################

#eliminamos la variable con el VIF el más alto
lin2<-lm(mpg~cyl+hp+drat+wt+qsec+gear+carb,data=mtcars2)
car::vif(lin2)
cal.vif2<-car::vif(lin2)
cal.vif2[,3]<-cal.vif2[,3]^2
cal.vif2
summary(lin2)
```

```{r}
#el VIF asociado a la variable "hp" sigue alto

lin3<-lm(mpg~cyl+drat+wt+qsec+gear+carb,data=mtcars2)
car::vif(lin3)
cal.vif3<-car::vif(lin3)
cal.vif3[,3]<-cal.vif3[,3]^2
cal.vif3
summary(lin3)
```
**Se puede eliminar otras variables de este modelo en una etapa posterior, pero en esta etapa hemos arreglado el problema de colinealidad**
**quizas seria un error eliminar "wt" porque es la variable la más significativa y la mas correlacionada con mpg. Podemos parar aquí o seguir eliminando "qsec"**

```{r}
lin4<-lm(mpg~cyl+drat+wt+gear+carb,data=mtcars2)
car::vif(lin4)
cal.vif4<-car::vif(lin4)
cal.vif4[,3]<-cal.vif4[,3]^2
cal.vif4
summary(lin4)#Ninguna variable parece pertinente en este modelo, salvo quizás la variable "carb". El modelo 3 es mejor.

```
Cuando modifico el lambda estoy penalizando el modelo, mientras no lo haga hago la regresión lineal común y silvestre.
```{r}
library(MASS)
lm.ridge(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2, lambda = 0)#NO ridge regression
h<-seq(14, 15, by=0.01)
rid<-lm.ridge(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2, lambda = h)#ridge regression
#rid$GCV
#rid$coef[,48]
#Para ubicar exactamente el punto optimal y aplicarlo 
```

```{r}
plot(h,rid$GCV)#GCV Validación Cruzada Generalizada
```


```{r}
rid<-lm.ridge(mpg~cyl+disp+hp+drat+wt+qsec+gear+carb,data=mtcars2, lambda = 14.653)
rid$coef#estimación con el parametro "ridge" optimal


```







---

#### LASSO (Least Absolute Shrinkage and Selection Operator)

Penaliza la suma de los valores absolutos de los coeficientes, lo que puede llevar a que algunos coeficientes se reduzcan exactamente a cero (selección de variables):

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\}
$$
Buscamos solucionar:
- El problema de colinealidad de alta dimensión (muchos predictores y observaciones)


---

#### Elastic Net

Es una combinación entre Ridge y LASSO, ponderada por un parámetro de mezcla \( \alpha \in [0, 1] \):

$$
\min_{\beta} \left\{ \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 + \lambda \left[ \alpha \sum_{j=1}^{p} |\beta_j| + (1 - \alpha) \sum_{j=1}^{p} \beta_j^2 \right] \right\}
$$

- Si \( \alpha = 1 \), se obtiene LASSO.
- Si \( \alpha = 0 \), se obtiene Ridge.

---

En R, estos modelos se pueden ajustar con la función `glmnet()` del paquete `glmnet`. Por ejemplo:

```r
library(glmnet)
glmnet(x, y, alpha = 0)   # Ridge
glmnet(x, y, alpha = 1)   # LASSO
glmnet(x, y, alpha = 0.5) # Elastic Net
```

\newpage

### ¿Qué hacer cuando la regularización no soluciona la Colinealidad?

En ciencia de datos, cuando enfrentamos **alta multicolinealidad** o un número elevado de predictores, podemos utilizar modelos que combinan regresión con **reducción de dimensionalidad**, como:

- **PCR** (Principal Component Regression)
- **PLS** (Partial Least Squares)

**¿Por qué no usar directamente LASSO o Ridge?**

LASSO y Ridge buscan reducir el sobreajuste penalizando los coeficientes del modelo (regularización), pero:

- No transforman las variables predictoras.

- Si hay multicolinealidad fuerte, no eliminan completamente el problema, aunque lo mitigan.

Aquí es donde PCR y PLS entran como estrategias basadas en transformación de los datos.


#### 1. Principal Component Regression (PCR)

#### 📌 Idea

PCR aplica primero Análisis de Componentes Principales (PCA) sobre los predictores \( \mathbf{X} \), y luego utiliza los primeros \( k \) componentes para ajustar una regresión lineal sobre la variable respuesta \( y \).PCR no considera la variable respuesta $y$ al crear los componentes. Es decir, puede elegir componentes que explican mucha varianza en $X$, pero no necesariamente son relevantes para predecir $y$

#### 🧮 Ecuación

Sea:

- \( \mathbf{X} \): matriz de predictores
- \( \mathbf{W} \): vectores propios de PCA
- \( \mathbf{Z} = \mathbf{XW} \): componentes principales

Entonces el modelo es:

$$
y = \mathbf{Z} \beta + \epsilon
$$

> **Importante**: los componentes son seleccionados solo para maximizar la varianza de \( \mathbf{X} \), no su relación con \( y \).

Presenta problemas al hacer PCA, ya que pierdo poder predictivo (incremento el sesgo) al solo tomar los k componentes, ya que los n-k componentes restantes, siendo n el total de componentes principales generados, no están siendo incluidos y que sí aportan información predictiva. Pierdo bondad de ajuste.PCA elige los componentes que explican la mayor varianza en los regresores $X$, sin tener en cuenta si esos componentes están relacionados con la variable respuesta $Y$. 

---

\newpage


#### 2. Partial Least Squares Regression (PLS)

##### 📌 Idea

PLS también reduce la dimensionalidad, pero a diferencia de PCR, busca componentes que **maximizan simultáneamente** la varianza en \( \mathbf{X} \) y la covarianza con \( y \).

##### 🧮 Ecuación

El primer componente se define como:

$$
T_1 = \mathbf{X} w_1 \quad \text{con } w_1 = \arg\max \text{Cov}(\mathbf{X}w, y)^2
$$

Luego se ajusta:

$$
y = T \beta + \epsilon
$$

donde \( T = \mathbf{XW} \).

> **Ventaja**: al considerar \( y \) en la creación de los componentes, suele tener mejor desempeño predictivo que PCR.

---

### 🔍 Comparación

\[
\begin{array}{|l|c|c|}
\hline
\textbf{Característica} & \textbf{PCR} & \textbf{PLS} \\
\hline
Usa \ y \ para crear componentes & \text{No} & \text{Sí} \\
\hline
Reduce dimensionalidad & \text{Sí (PCA)} & \text{Sí (Covarianza)} \\
\hline
Manejo de multicolinealidad & \text{Sí} & \text{Mejor} \\
\hline
Enfoque predictivo & \text{Moderado} & \text{Alto} \\
\hline
Interpretabilidad & \text{Baja} & \text{Media} \\
\hline
\end{array}
\]

---


### 📌 Cuándo usar cada uno

- Multicolinealidad severa: **PLS** o **PCR**
- Muchas variables predictoras: **LASSO**, **PLS**
- Predicción pura: **PLS**
- Interpretabilidad: **Ridge**, **LASSO**



